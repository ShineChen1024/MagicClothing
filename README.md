# oms-Diffusion
This repository is the official implementation of OMS-Diffusion.

OMS-Diffusion is a branch version of [OOTDiffusion](https://github.com/levihsu/OOTDiffusion), unlike the original OOTDiffusion trains two Unet, OMS-Diffusion only train one Unet, which means a 24GB memory machine(3090 or 4090) is enough.

Refer to our Paper to get more details. [arxiv](https://arxiv.org/abs/2403.01779)

> **OMS-Diffusion: One More Step Diffusion is All You Need for Virtual Try-on**<br>


## News
ğŸ”¥ [2024/3/8] æœ¬é¡¹ç›®å¼€æºäº†768åˆ†è¾¨ç‡çš„æ¨¡å‹æƒé‡ [huggingface](https://huggingface.co/shinehugging/oms-diffusion)ã€‚åœ¨512æƒé‡ä¸Šï¼Œä½ å¯èƒ½ä¼šé€šè¿‡å¢åŠ å›¾åƒåˆ†è¾¨ç‡æ¥è·å–è‰¯å¥½çš„é¢éƒ¨è¡¨ç°ï¼Œä½†åœ¨é«˜åˆ†è¾¨æƒ…å†µä¸‹è¡£æœå¯èƒ½å¤±æ§ã€‚768æƒé‡èƒ½å¸®åŠ©ä½ ä¸ç”¨å†çº ç»“äºåˆ†è¾¨ç‡ä¸é¢éƒ¨ç»†èŠ‚ã€‚
åœ¨768ç‰ˆæœ¬ä¸­è¿˜è°ƒæ•´äº†è®­ç»ƒç­–ç•¥ï¼Œä½ å¯ä»¥å•ç‹¬æ§åˆ¶è¡£æœå¼ºåº¦å’Œæç¤ºè¯å¼ºåº¦äº†ã€‚æ­¤æƒé‡é»˜è®¤çš„[IPadapter-faceID](https://huggingface.co/h94/IP-Adapter-FaceID) ç‰ˆæœ¬ä¼šæ˜¯FaceIDPlusV2ã€‚


We released the weights trained on 768 resolution [huggingface](https://huggingface.co/shinehugging/oms-diffusion). At 512 weights, you may get good facial performance by increasing image resolution, but in high-resolution situations, clothing may lose control. 768 weights can help you not to worry about how to balance the resolution and facial details.
In this version, the training strategy has also been adjusted, allowing you to independently control the intensity of clothing and prompts. The default version of [IPadapter faceID](https://huggingface.co/h94/IP-Adapter-FaceID) is FaceIDPlusV2.

ğŸ”¥ [2024/2/28] æœ¬é¡¹ç›®æ”¯æŒ[IPadapter-faceID](https://huggingface.co/h94/IP-Adapter-FaceID) ç»“åˆ controlnet_openposeï¼ä½ èƒ½é€šè¿‡è‚–åƒä¸å§¿åŠ¿å‚è€ƒå›¾è¿›è¡Œè¯•ç©¿ã€‚
åœ¨æˆ‘ä»¬çš„æµ‹è¯•ä¸­ï¼Œè‚–åƒç›¸ä¼¼åº¦ FaceIDPlus > FaceIDPlusV2 > FaceID, æ‰€ä»¥IPadapter-faceIDé»˜è®¤ç‰ˆæœ¬å°†ä¼šä»FaceIDPlusV2è½¬ä¸ºFaceIDPlus.

We support [IPadapter-faceID](https://huggingface.co/h94/IP-Adapter-FaceID) with controlnet_openposeï¼You can try virtual try-on by uploading a portrait and a reference pose image.
In our testing, the portrait similarity among the three version is FaceIDPlus > FaceIDPlusV2 > FaceID, so we change the default FaceIDPlusV2 version to FaceIDPlus.

Try __gradio_ipadapter_openpose.py__

ğŸ”¥ ğŸ”¥ [2024/2/23] ç°åœ¨æœ¬é¡¹ç›®æ”¯æŒ[IPadapter-faceID](https://huggingface.co/h94/IP-Adapter-FaceID)ï¼ä½ èƒ½é€šè¿‡è‚–åƒè¿›è¡Œè¯•ç©¿ã€‚å¢åŠ å›¾åƒé«˜åº¦èƒ½é¿å…å¤§å¤´å¨ƒå¨ƒæƒ…å†µã€‚

We support [IPadapter-faceID](https://huggingface.co/h94/IP-Adapter-FaceID) nowï¼You can try virtual try-on by uploading a portrait. Enlarge height will alleviate weird big-head result.

Have fun with __gradio_ipadapter_faceid.py__


## Demos
__IPadapter-faceID + controlnet_openpose demo__
![demo5](fig/figure5.jpg)&nbsp;

__IPadapter-faceID demo__
![demo4](fig/figure4.png)&nbsp;

__generative demo__
![demo1](fig/figure1.png)&nbsp;

__controlnet demo__ (openpose, inpainting)
![demo2](fig/figure2.png)&nbsp;

__some unexpected funny application__ (need more effort)
![demo3](fig/figure3.png)&nbsp;

## Tips
æœ¬é¡¹ç›®è‡´åŠ›äºååŠ©ä¸­å°æœè£…å•†å®¶è§£å†³çº¦æ‹æ¨¡ç‰¹å¯¼è‡´çš„é«˜æ˜‚æˆæœ¬é—®é¢˜ã€‚
åªéœ€ä¸€å¼ æœè£…å›¾ç‰‡ï¼Œå¯ä»¥ç”Ÿæˆå„ç§èº«ç©¿è¯¥æœè£…çš„æ¨¡ç‰¹ã€‚
åæœŸä¹Ÿä¼šä¸ºä¸ªäººç”¨æˆ·æä¾›æœè£…è¯•ç©¿åŠŸèƒ½ã€‚
> This project is committed to assisting small-sized clothing businesses in solving the high cost problem caused by model appointments.
> With just one clothing image, various models wearing that clothing can be generated
> In the later stage, it will also be provided for individual users for picking suitable garment.


## Guidance
1ã€é»˜è®¤çš„æç¤ºè¯æ˜¯"a photography of a model", ä½ å¯ä»¥é€šè¿‡å¢åŠ è¯æ±‡åƒ "sunglasses" æˆ–è€… "red skirt" æ¥è®¾å®šæ¨¡ç‰¹ï¼Œæˆ–è€…æŠŠ"model"æ¢æˆ"man"æ¥æŒ‡å®šæ€§åˆ«ï¼Œä½†æ˜¯ä½œè€…ä¹Ÿä¸æ¸…æ¥šå®ƒå¯¹æœ€ç»ˆçš„è¯•è¡£ç»“æœæœ‰ä½•å½±å“ã€‚

The default prompt is "a photography of a model". You can add words like "sunglasses", "red skirt" or change "model" to "man" to specify the model, but we do not know how it will affect the final result.

2ã€512æƒé‡çš„é»˜è®¤çš„å›¾åƒåˆ†è¾¨ç‡æ˜¯ï¼ˆ512ï¼Œ384ï¼‰ï¼Œæ­¤åˆ†è¾¨ç‡å¯èƒ½ä¼šç”Ÿæˆæ‰­æ›²çš„é¢éƒ¨ã€‚ä½ å¯ä»¥å¢åŠ å›¾åƒåˆ†è¾¨ç‡æ¥è·å¾—ä¼˜ç§€çš„ç»†èŠ‚ï¼Œä½†æ˜¯æœ‰äº›æƒ…å†µä¼šå¯¼è‡´è¡£æœå¤±æ§,éœ€è¦å¤šæ¬¡å°è¯•ä¸åŒéšæœºç§å­æ‰èƒ½å¾—åˆ°è¾ƒå¥½çš„ç»“æœã€‚

The default image resolution for 512 weights is (512,384), which may generate distorted faces. You can increase the image resolution to obtain excellent details, but in some cases, it can lead to clothing losing control.

3ã€å¦‚æœä½ å‘ç°è¡£æœç»†èŠ‚å’Œä½ æœŸæœ›çš„å¯¹åº”ä¸ä¸Šï¼Œè¯·å°è¯•ä¸åŒçš„éšæœºç§å­ã€‚

If you find that the details of the clothes do not match your expectations, please try different random seeds.

4ã€guidance Scaleæ¥æ§åˆ¶è¯•è¡£æœè£…çš„å¼ºåº¦ï¼Œä½†è¾ƒå¤§çš„å€¼ä¼šå¯¼è‡´å›¾åƒå¤±çœŸã€‚æ¨è2.0å·¦å³ã€‚

Guidance Scale is used to control the strength of clothes, but larger values may cause image distortion. (1.0 , 3.0) may be a good interval.

5ã€æœ¬é¡¹ç›®å†…ç½®äº†ä¸€ä¸ªå°†æœè£…ä»èƒŒæ™¯åˆ†ç¦»å‡ºæ¥çš„æƒé‡ï¼Œä½†æ˜¯å®ƒå¯èƒ½ä¸æ˜¯æœ€ä¼˜çš„ï¼Œå¦‚æœä½ æœ‰æ›´å¥½çš„æƒé‡ï¼Œåƒ[SAM](https://github.com/facebookresearch/segment-anything)ï¼Œè¯·æ›¿æ¢å®ƒã€‚

This project contains a module that separates clothing from the background, but it may not be optimal. if you have better weights like [SAM](https://github.com/facebookresearch/segment-anything). Please replace it.


Anyway, have fun with it.



## WEIGHTS
- [x] 512 resolution weights: [huggingface](https://huggingface.co/shinehugging/oms-diffusion) or [Baidu Drive](https://pan.baidu.com/s/1UJgARIfXyZz5AyLUWYEWgg?pwd=ae6f)

è¯¥æƒé‡æ˜¯ä¸€ä¸ªå®éªŒæ€§è´¨çš„ç‰ˆæœ¬ï¼Œåœ¨VITON-HDä¸€ä¸‡å¤šå¼ è®­ç»ƒé›†ï¼ˆå¥³æ¨¡ç‰¹ï¼Œä¸ŠåŠèº«ï¼Œå¤è£…ï¼Œ4:3åˆ†è¾¨ç‡ï¼‰ä¸Šè®­ç»ƒï¼Œæ‰€ä»¥å®ƒå¯¹æŸäº›é¢œè‰²æˆ–ç±»åˆ«å¯èƒ½æœ‰æ‰€åå¥½ã€‚è®­ç»ƒåˆ†è¾¨ç‡ä¸ºï¼ˆ512,384ï¼‰ï¼Œå¯¹è¾ƒå°çš„å­—ä½“å¯èƒ½ä¼šå¯¼è‡´æ¨¡ç³Šå˜å½¢ï¼ˆåœ¨é«˜åˆ†è¾¨ç‡ä¸‹å·²è§£å†³ï¼‰ã€‚

This weight is an experimental model trained on over 10000 VITON-HD training images (female models, upper body, summer clothing, 4:3 resolution), so it may have preferences for certain colors or categories.The training resolution is (512,384) which may result in blurry deformation for smaller fonts (resolved at high resolutions).

- [x] 768 resolution weightsï¼ˆmaybe early March, if we get more starsï¼‰

æˆ‘ä»¬åœ¨768åˆ†è¾¨ç‡ä¸Šè°ƒä¼˜è®­ç»ƒç­–ç•¥ï¼Œå¢åŠ æ¨¡å‹åˆ†è¾¨ç‡ï¼Œæ‹“å±•è¡£æœç±»åˆ«å’Œæ¨¡ç‰¹ç±»åˆ«ï¼Œæ•¬è¯·æœŸå¾…ã€‚

We optimize the training strategy at 768 resolution, increase model resolution, and expand clothing and model categories. Stay informed with us! 

- [ ] 1024 resolution weights (add dress lower-body, garment)

## Installation

1. Clone the repository

```sh
git clone https://github.com/chenshine1/oms-Diffusion.git
```

2. Create a conda environment and install the required packages

```sh
conda create -n oms-diffusion python==3.10
conda activate oms-diffusion
pip install torch==2.0.1 torchvision==0.15.2 numpy==1.25.1 diffusers==0.25.1 opencv-python==4.9.0.80  transformers==4.31.0 gradio==4.16.0 safetensors==0.3.1 controlnet-aux==0.0.6 accelerate==0.21.0
```

## Inference
1. python demo

512 weights
```sh
python inference.py --cloth_path[your cloth path] --model_path[your model path]
```

768 weights
```sh
python inference.py --cloth_path[your cloth path] --model_path[your model path] --enable_cloth_guidance
```
2. gradio demo

512 weights

```sh
python gradio_generate.py --model_path[your model path] 
```
768 weights

```sh
python gradio_generate.py --model_path[your model path] --enable_cloth_guidance
```

## TODO List
- [x] Inference code
- [x] Gradio demo
- [x] Support Controlnet
- [x] Support IP-adapter-faceid
- [x] Release 512 resolution weights
- [x] Release 768 resolution weights
- [ ] Release 1024 resolution weight
- [ ] Support lower-body clothes
- [ ] Support full-body dresses
- [ ] Support SD-inpainting weight
- [ ] Support SDXL
- [ ] Support InstantID
- [ ] Support video virtual-try0n
